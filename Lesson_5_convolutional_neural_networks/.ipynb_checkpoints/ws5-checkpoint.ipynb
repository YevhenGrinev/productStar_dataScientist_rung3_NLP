{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определяем, нужно ли ставить тег \"кот\"\n",
    "\n",
    "Пользователи должны помечать посты на сайте специальными тегами, описывающими контент, содержащийся в посте. Наша задача состоит в том, чтобы помочь им в этом. В частности, анализируя картинки в посте, нужно определить, содержится ли на картинке кот. \n",
    "\n",
    "Для решения этой задачи мы будем использовать свёрточную нейросеть. Но мы не будем тренировать её с нуля, а воспользуемся уже предобученной ResNet на датасете Imagenet. Мы произведём transfer-learning и дообучим её на нашем небольшом датасете для того, чтобы она была заточена именно под определение котов.\n",
    "\n",
    "На примере этой задачи мы познакомимся со свёрточной архитектурой, освоим transfer-learning и онлайн-аугментации в Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# как обычно, фиксируем сид\n",
    "np.random.seed(17)\n",
    "tf.random.set_seed(17)\n",
    "random.seed(17)\n",
    "\n",
    "# и ограничим потребление видеопамяти\n",
    "GPUs = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(GPUs[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Keras уже имеется несколько распространённых предобученных моделей нейросетей. Их список можно посмотреть здесь: https://keras.io/api/applications/\n",
    "\n",
    "Давайте воспользуемся обычной ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = tf.keras.applications.ResNet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 1000)         2049000     avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 25,636,712\n",
      "Trainable params: 25,583,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# изучим её архитектуру\n",
    "\n",
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\g0nzalez\\venv38\\lib\\site-packages (8.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\g0nzalez\\venv38\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# для корректной работы с картинками загрузим модуль pillow\n",
    "\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, как вообще работает ResNet. Для примера скормим ей обычную картинку с котиком. \n",
    "\n",
    "Для загрузки картинок в Keras предусмотрен специальный инструмент, в котором сразу же можно указать целевое разрешение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_img = tf.keras.preprocessing.image.load_img('cat.jpg', target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим, что получилось\n",
    "\n",
    "cat_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# далее необходимо представить картинку в виде массива\n",
    "\n",
    "cat_img = tf.keras.preprocessing.image.img_to_array(cat_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 канала 224х224 пикселя\n",
    "\n",
    "cat_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. на вход нейросети нужно подавать массив картинок, то сначала сделаем массив из одной картинки, а затем специально подготовим его для ResNet функцией `preprocess_input`, которая отмасштабирует значения пикселей на нужную величину."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_img = np.expand_dims(cat_img, axis=0)\n",
    "cat_img = tf.keras.applications.resnet.preprocess_input(cat_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[146.061  , 100.221  ,  60.32   ],\n",
       "         [144.061  ,  98.221  ,  58.32   ],\n",
       "         [144.061  ,  98.221  ,  58.32   ],\n",
       "         ...,\n",
       "         [147.061  , 104.221  ,  70.32   ],\n",
       "         [147.061  , 104.221  ,  70.32   ],\n",
       "         [147.061  , 104.221  ,  70.32   ]],\n",
       "\n",
       "        [[145.061  ,  99.221  ,  59.32   ],\n",
       "         [144.061  ,  98.221  ,  58.32   ],\n",
       "         [144.061  ,  98.221  ,  58.32   ],\n",
       "         ...,\n",
       "         [147.061  , 104.221  ,  70.32   ],\n",
       "         [147.061  , 104.221  ,  70.32   ],\n",
       "         [147.061  , 104.221  ,  70.32   ]],\n",
       "\n",
       "        [[145.061  ,  99.221  ,  59.32   ],\n",
       "         [144.061  ,  98.221  ,  58.32   ],\n",
       "         [144.061  ,  98.221  ,  58.32   ],\n",
       "         ...,\n",
       "         [147.061  , 104.221  ,  70.32   ],\n",
       "         [147.061  , 104.221  ,  70.32   ],\n",
       "         [147.061  , 104.221  ,  70.32   ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[149.061  , 125.221  ,  98.32   ],\n",
       "         [150.061  , 125.221  , 100.32   ],\n",
       "         [150.061  , 125.221  , 100.32   ],\n",
       "         ...,\n",
       "         [149.061  , 134.22101, 126.32   ],\n",
       "         [149.061  , 134.22101, 126.32   ],\n",
       "         [149.061  , 134.22101, 126.32   ]],\n",
       "\n",
       "        [[149.061  , 125.221  ,  98.32   ],\n",
       "         [149.061  , 124.221  ,  99.32   ],\n",
       "         [150.061  , 125.221  , 100.32   ],\n",
       "         ...,\n",
       "         [149.061  , 134.22101, 126.32   ],\n",
       "         [149.061  , 134.22101, 126.32   ],\n",
       "         [149.061  , 134.22101, 126.32   ]],\n",
       "\n",
       "        [[149.061  , 125.221  ,  98.32   ],\n",
       "         [149.061  , 124.221  ,  99.32   ],\n",
       "         [150.061  , 125.221  , 100.32   ],\n",
       "         ...,\n",
       "         [149.061  , 134.22101, 126.32   ],\n",
       "         [149.061  , 134.22101, 126.32   ],\n",
       "         [149.061  , 134.22101, 126.32   ]]]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем предсказание\n",
    "\n",
    "pred = resnet.predict(cat_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.30767023e-06, 5.08551739e-06, 3.60302278e-04, 9.82225683e-05,\n",
       "        5.55646739e-06, 1.89638318e-04, 3.72714976e-06, 3.24352936e-06,\n",
       "        1.08489157e-05, 2.26526618e-06, 7.47342256e-06, 3.40192145e-07,\n",
       "        4.18389027e-06, 5.36159896e-05, 1.03922639e-06, 7.34723289e-05,\n",
       "        2.26790553e-06, 1.12883417e-05, 1.23069651e-04, 2.35791140e-06,\n",
       "        4.35606926e-05, 9.39421170e-06, 9.43851137e-06, 1.18909338e-05,\n",
       "        2.90087700e-01, 1.32962180e-06, 1.11726138e-06, 4.82717667e-07,\n",
       "        7.83330051e-07, 1.77878258e-06, 1.06428593e-06, 4.15006480e-06,\n",
       "        3.59363213e-07, 1.06543612e-05, 1.86802354e-05, 2.37400582e-06,\n",
       "        4.75482266e-05, 3.49009696e-07, 3.42489238e-06, 2.67734231e-06,\n",
       "        2.70726218e-06, 1.71914610e-06, 1.68194424e-06, 4.77355534e-05,\n",
       "        1.71260717e-06, 3.14767249e-06, 2.08026017e-06, 1.58752828e-05,\n",
       "        2.39898309e-05, 1.29838759e-06, 1.06505149e-06, 9.10020462e-05,\n",
       "        1.03514658e-07, 9.21833362e-07, 2.65716108e-06, 1.24104338e-06,\n",
       "        2.99108535e-07, 3.65515604e-07, 4.89563320e-07, 3.64032246e-07,\n",
       "        2.21696541e-06, 5.55292615e-07, 8.61113563e-07, 2.12524100e-07,\n",
       "        4.69755378e-07, 9.47988553e-07, 1.39364041e-04, 1.91582080e-06,\n",
       "        6.34218804e-06, 1.12598820e-04, 3.42840872e-06, 7.58869464e-06,\n",
       "        5.05315984e-06, 3.54827284e-06, 3.44497676e-06, 8.22298034e-07,\n",
       "        5.59427917e-07, 2.66204597e-06, 1.73868357e-05, 2.18868718e-06,\n",
       "        7.39984216e-06, 2.78761422e-06, 1.47090486e-05, 9.91627239e-05,\n",
       "        7.22798688e-07, 1.08047294e-04, 1.63528894e-05, 3.61721613e-05,\n",
       "        4.40857293e-05, 1.80308070e-05, 1.76821345e-06, 1.01379477e-07,\n",
       "        3.46701768e-07, 1.99044536e-07, 5.41467416e-05, 8.09525034e-08,\n",
       "        4.48687842e-08, 1.61282205e-05, 1.56902934e-06, 2.98378654e-05,\n",
       "        2.19574194e-05, 1.18142843e-05, 2.84780867e-07, 1.00643651e-04,\n",
       "        2.99692936e-02, 1.43740084e-02, 3.30072292e-03, 8.31991770e-07,\n",
       "        1.14373449e-06, 5.77465812e-07, 7.67298661e-06, 6.30070508e-06,\n",
       "        1.40160546e-06, 5.32195736e-06, 8.86624150e-07, 6.90054605e-07,\n",
       "        1.09676068e-06, 4.54086876e-06, 6.42325062e-07, 1.51477366e-07,\n",
       "        3.40671050e-07, 1.19768828e-07, 7.90787965e-07, 1.50145627e-06,\n",
       "        8.27330314e-06, 2.88848037e-06, 2.39599581e-06, 2.31405352e-07,\n",
       "        2.07700012e-07, 8.77088951e-07, 3.26763285e-07, 5.15045883e-07,\n",
       "        2.31779381e-06, 5.17569902e-07, 2.92479285e-06, 1.68064548e-06,\n",
       "        2.10279068e-05, 5.63738722e-05, 1.47310959e-04, 6.95372648e-07,\n",
       "        1.48123502e-06, 1.89542206e-05, 3.48557660e-05, 8.08805180e-06,\n",
       "        4.59121475e-06, 8.47912044e-04, 8.89698058e-05, 3.45922344e-05,\n",
       "        6.17467740e-05, 1.62791577e-04, 9.36199818e-03, 4.94614251e-05,\n",
       "        5.63610001e-06, 6.47382376e-06, 6.45615273e-06, 1.58637213e-05,\n",
       "        2.27200121e-06, 6.75662795e-06, 9.08762813e-05, 3.13806777e-05,\n",
       "        3.86900297e-07, 2.77552971e-07, 2.70431019e-06, 4.33703235e-06,\n",
       "        2.54508154e-06, 1.54366990e-06, 5.67858751e-06, 4.29779357e-06,\n",
       "        4.42503224e-04, 3.08357784e-08, 9.51521827e-07, 1.46535613e-05,\n",
       "        4.03265904e-06, 2.80631666e-05, 1.04973968e-02, 5.83283452e-07,\n",
       "        3.02571301e-08, 1.52347855e-06, 2.13726060e-04, 3.76262440e-04,\n",
       "        2.92403600e-03, 2.22514354e-04, 1.88064150e-05, 1.20432815e-03,\n",
       "        3.89830166e-06, 9.58819783e-06, 1.31149092e-04, 2.17379620e-05,\n",
       "        2.19776593e-05, 8.98589278e-05, 2.78956250e-05, 1.69697742e-05,\n",
       "        3.71202259e-05, 1.01451595e-04, 4.09165068e-06, 5.00863651e-03,\n",
       "        1.50344022e-05, 6.82076425e-05, 1.88920792e-04, 2.51965626e-04,\n",
       "        6.34957178e-06, 2.79908108e-05, 9.61649494e-06, 1.84206830e-04,\n",
       "        6.43703288e-06, 1.13425249e-05, 1.93396027e-05, 7.32565525e-07,\n",
       "        8.15380918e-05, 9.18390651e-06, 7.75123044e-05, 2.12446521e-05,\n",
       "        5.41092106e-07, 4.54401572e-07, 4.31757826e-06, 1.34126856e-06,\n",
       "        1.02859376e-05, 1.36834637e-06, 3.07261217e-07, 1.41819419e-05,\n",
       "        6.83485814e-06, 2.84092694e-05, 1.32707346e-05, 2.20778544e-04,\n",
       "        1.20042474e-04, 2.97403807e-04, 4.61746049e-06, 2.04838958e-04,\n",
       "        5.50378627e-06, 2.34221420e-06, 1.30715534e-06, 3.17583795e-06,\n",
       "        1.82311817e-06, 5.34017636e-05, 1.84354183e-04, 1.49442654e-04,\n",
       "        3.22026353e-05, 7.26778744e-05, 2.24191535e-06, 1.42155977e-06,\n",
       "        4.12523241e-06, 9.79471315e-06, 1.80233119e-03, 5.80482476e-04,\n",
       "        4.50234438e-06, 1.20665394e-01, 5.07444725e-04, 5.96092650e-05,\n",
       "        1.05124584e-03, 1.81973708e-04, 3.82873579e-04, 2.06804489e-05,\n",
       "        2.92564237e-05, 6.11121161e-03, 8.28284363e-04, 3.54233907e-06,\n",
       "        2.90679076e-04, 1.43074521e-05, 1.39841386e-05, 2.63729225e-06,\n",
       "        9.87608731e-03, 2.04944208e-05, 2.92447512e-04, 2.07931116e-05,\n",
       "        1.73511235e-05, 1.91440085e-05, 8.06940625e-06, 3.97817130e-05,\n",
       "        4.52564703e-03, 1.39052584e-03, 2.71384791e-03, 1.59830449e-03,\n",
       "        2.67736590e-03, 1.16053934e-03, 2.69796419e-05, 9.08333968e-05,\n",
       "        1.78555504e-03, 1.02551778e-04, 4.45028534e-03, 1.52148656e-03,\n",
       "        1.27639261e-03, 2.25396082e-01, 3.21585834e-02, 5.21659153e-03,\n",
       "        2.50572432e-03, 9.18909311e-02, 8.40783399e-03, 9.09964927e-03,\n",
       "        9.66866792e-05, 5.31778205e-04, 9.78954020e-04, 3.81826139e-05,\n",
       "        2.74423946e-04, 2.11901606e-05, 1.91466723e-04, 4.73260996e-04,\n",
       "        8.18964094e-04, 3.41517966e-06, 1.31665275e-03, 3.72574636e-04,\n",
       "        1.20659848e-07, 1.80535778e-06, 8.63455989e-06, 4.63284096e-06,\n",
       "        5.94029189e-07, 1.21611583e-06, 2.15292966e-05, 2.58460732e-05,\n",
       "        3.36580592e-06, 3.59361320e-06, 1.00449433e-05, 3.92026004e-06,\n",
       "        7.98700626e-07, 2.15366777e-06, 2.86890076e-06, 5.87855425e-07,\n",
       "        8.58962539e-06, 1.79220388e-06, 1.60970342e-06, 1.11829422e-06,\n",
       "        1.02832246e-06, 4.28040948e-06, 6.79642180e-05, 1.41547403e-07,\n",
       "        4.75277102e-06, 4.96268456e-07, 7.62205036e-06, 5.32976155e-05,\n",
       "        5.84721283e-06, 9.42575662e-06, 4.64439596e-04, 1.49746041e-03,\n",
       "        2.88643583e-04, 1.08091299e-04, 6.08573316e-07, 9.30026581e-04,\n",
       "        4.96590874e-05, 2.01211311e-04, 2.70848512e-04, 2.85434658e-06,\n",
       "        1.46618710e-04, 6.25891262e-04, 3.13942059e-04, 1.87274327e-05,\n",
       "        1.83627490e-04, 1.36296039e-05, 4.20327524e-06, 6.47382694e-05,\n",
       "        1.77702619e-04, 8.26238283e-06, 3.60295758e-06, 1.62348073e-07,\n",
       "        1.46234265e-06, 4.46348025e-07, 6.99093926e-06, 1.62747256e-05,\n",
       "        9.54770730e-05, 1.24426108e-04, 2.59332637e-06, 4.13313228e-06,\n",
       "        9.63059574e-05, 6.71520002e-06, 7.63693461e-05, 1.98434992e-03,\n",
       "        1.93510059e-05, 1.83038384e-04, 6.64643303e-05, 2.63249422e-05,\n",
       "        1.28971506e-05, 2.96732214e-05, 1.28608395e-03, 2.93315461e-05,\n",
       "        4.42224555e-05, 9.12726973e-05, 8.93921970e-06, 3.79798371e-06,\n",
       "        1.06407686e-06, 2.44697527e-04, 2.32061284e-05, 1.30668064e-04,\n",
       "        1.00948040e-04, 3.90340892e-06, 9.36492143e-05, 8.59858061e-04,\n",
       "        1.47747181e-04, 1.71835854e-05, 1.63735729e-06, 7.16240152e-07,\n",
       "        2.03446489e-05, 4.66137317e-06, 1.72935615e-04, 8.94078403e-05,\n",
       "        3.77538049e-06, 1.20149275e-06, 5.60693843e-05, 3.06029938e-06,\n",
       "        6.85782481e-07, 6.34747039e-06, 1.92291350e-06, 5.51990797e-06,\n",
       "        5.31514070e-06, 3.63700508e-07, 9.56442932e-07, 1.31575143e-06,\n",
       "        1.30848741e-06, 3.80693336e-06, 1.30521030e-05, 7.79092431e-07,\n",
       "        5.90383979e-06, 1.51657776e-04, 4.05757788e-07, 3.63307408e-05,\n",
       "        3.82742328e-05, 5.20404683e-06, 1.02179838e-04, 3.25740120e-06,\n",
       "        3.37662777e-06, 5.39105713e-06, 1.07737224e-05, 4.02392507e-05,\n",
       "        1.66435329e-05, 1.03410175e-05, 4.15287359e-05, 2.98870327e-06,\n",
       "        3.52460734e-06, 1.34775346e-05, 3.43403750e-04, 1.32243194e-05,\n",
       "        9.66489606e-05, 9.42247607e-06, 2.01092644e-06, 2.75462662e-05,\n",
       "        7.03683236e-06, 4.69514680e-06, 1.21816481e-02, 5.50689548e-03,\n",
       "        2.66285213e-07, 7.34335913e-07, 5.47982518e-06, 9.28634290e-06,\n",
       "        1.19662029e-04, 1.93197206e-06, 4.91165747e-06, 1.44651264e-03,\n",
       "        4.95503741e-07, 8.59095326e-06, 4.13336465e-06, 4.29692700e-05,\n",
       "        4.67479958e-06, 1.45865926e-07, 4.75431671e-06, 1.86972029e-05,\n",
       "        1.86779987e-04, 2.66537518e-05, 1.81474425e-05, 9.09806568e-06,\n",
       "        7.16679569e-05, 2.22905586e-03, 4.59576677e-06, 8.33779359e-06,\n",
       "        5.66124363e-05, 9.36208653e-06, 1.14671902e-05, 7.48143473e-04,\n",
       "        1.96007404e-05, 3.84879932e-05, 9.67398410e-06, 3.06868083e-06,\n",
       "        9.84893813e-06, 6.25768225e-05, 1.38186748e-04, 1.15243847e-05,\n",
       "        2.18130003e-06, 5.16346227e-05, 8.21526846e-05, 6.72963824e-06,\n",
       "        1.80711663e-06, 4.93075204e-05, 2.24497096e-04, 1.94514905e-05,\n",
       "        9.32099920e-06, 8.64239837e-07, 2.02655360e-06, 1.89075527e-05,\n",
       "        9.89489536e-07, 2.53620624e-06, 4.83964527e-07, 1.31396460e-04,\n",
       "        4.77751310e-05, 2.41523339e-05, 3.93117552e-05, 2.94081510e-05,\n",
       "        6.30652212e-05, 7.61322724e-07, 1.64796529e-05, 1.28582360e-06,\n",
       "        1.80270843e-04, 2.49956283e-05, 3.62346545e-05, 1.51831864e-05,\n",
       "        3.89583903e-07, 9.30132592e-05, 2.99879466e-05, 4.59364128e-06,\n",
       "        2.13265193e-05, 3.63494910e-05, 5.01050408e-06, 6.53046584e-07,\n",
       "        2.04147829e-04, 9.62116701e-07, 2.94934057e-06, 2.02921939e-07,\n",
       "        1.79375074e-05, 5.33544016e-06, 4.80463023e-05, 4.01866535e-04,\n",
       "        5.31303995e-05, 3.84670420e-05, 1.31804461e-06, 1.29543105e-05,\n",
       "        1.78792343e-05, 1.08747861e-06, 7.25960263e-06, 8.13197494e-06,\n",
       "        1.05116915e-05, 1.35986954e-06, 6.99289967e-06, 1.78627361e-05,\n",
       "        2.09068694e-05, 1.28796484e-04, 1.06677198e-05, 1.39774738e-05,\n",
       "        2.77405456e-06, 5.45994544e-06, 1.73500321e-05, 6.25574842e-07,\n",
       "        7.72376916e-06, 3.88122462e-05, 1.60786078e-06, 3.20231891e-04,\n",
       "        4.21612594e-06, 2.57046418e-06, 1.06786420e-05, 2.00100185e-04,\n",
       "        1.95083476e-06, 8.00701091e-05, 5.68993300e-06, 6.05479670e-07,\n",
       "        1.10353005e-06, 3.13532146e-05, 4.33836431e-06, 5.63294307e-05,\n",
       "        2.52077385e-04, 4.07827501e-05, 3.13107967e-06, 8.14753434e-07,\n",
       "        5.08197809e-06, 7.35235335e-06, 3.00018337e-06, 2.11484598e-06,\n",
       "        8.30740646e-07, 9.52436153e-07, 1.15651614e-03, 1.33669137e-05,\n",
       "        1.06399159e-06, 7.61538058e-06, 3.06885522e-05, 1.14059048e-05,\n",
       "        1.36664376e-05, 2.64517917e-06, 2.04462394e-05, 2.46633977e-06,\n",
       "        6.08399896e-05, 6.44917804e-07, 1.18531634e-05, 1.05811648e-06,\n",
       "        2.04116745e-07, 1.87662117e-05, 3.87671171e-05, 3.04864716e-06,\n",
       "        1.47259243e-06, 1.59536967e-06, 1.13367591e-07, 3.34598258e-06,\n",
       "        3.88181479e-06, 1.36955077e-05, 1.31793274e-06, 2.93997116e-04,\n",
       "        5.82954235e-05, 3.09159921e-04, 4.37870858e-06, 2.94560778e-06,\n",
       "        1.38068326e-05, 1.65723395e-05, 5.18795741e-06, 3.60396939e-07,\n",
       "        1.13690519e-04, 1.47121191e-05, 7.03868454e-06, 1.53718775e-05,\n",
       "        2.45070754e-04, 7.37427627e-06, 1.06166899e-05, 2.58672162e-06,\n",
       "        3.18239108e-05, 6.01820757e-05, 1.85228477e-04, 2.63874131e-06,\n",
       "        6.03264547e-04, 1.71583588e-06, 2.22823219e-05, 1.12030764e-04,\n",
       "        6.68679405e-08, 3.41401865e-05, 6.75107594e-06, 1.41498836e-04,\n",
       "        8.81636424e-06, 2.87176263e-05, 9.59402496e-06, 1.92832103e-05,\n",
       "        3.38902755e-05, 1.69505274e-05, 2.21756054e-04, 1.03638822e-05,\n",
       "        1.82095130e-06, 6.71249495e-07, 2.23447805e-05, 6.90883667e-07,\n",
       "        6.59312764e-06, 1.27387166e-05, 2.18467176e-05, 3.92784568e-06,\n",
       "        3.36716257e-05, 2.30331116e-05, 3.71092710e-06, 1.60197436e-04,\n",
       "        1.16160336e-05, 3.34629694e-05, 2.01917737e-05, 1.01027108e-05,\n",
       "        1.77489455e-05, 6.75479214e-06, 2.91266520e-06, 1.26093393e-04,\n",
       "        2.36752949e-05, 1.02845979e-06, 3.79106277e-05, 1.32124196e-05,\n",
       "        8.36046274e-06, 1.87215497e-04, 9.24245396e-05, 4.72623879e-06,\n",
       "        3.03346060e-05, 8.55558301e-06, 7.89894420e-06, 8.36173058e-06,\n",
       "        7.48847185e-07, 1.70178591e-05, 1.05217204e-03, 7.67015536e-06,\n",
       "        1.30191518e-06, 6.43450377e-08, 2.87447256e-05, 4.75994284e-06,\n",
       "        6.80182275e-05, 2.69213342e-06, 3.93500159e-05, 8.73954923e-06,\n",
       "        1.74276670e-06, 9.39066649e-06, 8.76340255e-06, 3.19552612e-07,\n",
       "        8.84839324e-07, 1.17507981e-04, 2.92387587e-04, 4.73903447e-06,\n",
       "        3.58984718e-04, 1.90543578e-05, 4.00522413e-06, 2.14614647e-05,\n",
       "        2.25716896e-04, 1.97842703e-04, 1.18564185e-05, 3.86124930e-06,\n",
       "        5.48082098e-05, 4.49861864e-06, 5.62961031e-06, 7.10972472e-08,\n",
       "        7.79119546e-06, 1.91706822e-05, 2.32410088e-07, 1.53403762e-05,\n",
       "        7.84090153e-05, 1.24164417e-05, 7.68523205e-06, 3.16757287e-05,\n",
       "        2.65154777e-05, 8.08487093e-05, 5.33415232e-06, 9.73971964e-06,\n",
       "        1.39942262e-04, 1.64126413e-06, 4.73962155e-06, 4.76501100e-06,\n",
       "        8.70204531e-05, 4.40119663e-07, 5.76748971e-06, 1.53892063e-06,\n",
       "        1.10414912e-04, 3.81444006e-05, 1.33533322e-04, 1.85808967e-05,\n",
       "        5.14200765e-05, 8.20828518e-06, 5.17783383e-06, 2.47769822e-06,\n",
       "        2.82226174e-05, 1.55798648e-06, 4.30942919e-06, 3.30061448e-04,\n",
       "        3.90667374e-05, 1.47286931e-03, 4.18841591e-05, 1.14633513e-05,\n",
       "        2.17900060e-06, 4.26527957e-04, 2.23906611e-07, 5.98930342e-07,\n",
       "        8.92649696e-05, 1.22048041e-05, 2.01412786e-06, 1.02165599e-04,\n",
       "        1.07727874e-05, 1.08885106e-05, 6.90629705e-08, 3.27150883e-05,\n",
       "        3.09025177e-06, 4.10626672e-06, 5.96058562e-05, 3.05479102e-06,\n",
       "        5.66378149e-05, 1.19899460e-05, 5.30015022e-05, 4.10156235e-06,\n",
       "        2.42516271e-05, 4.53192843e-05, 2.70813907e-05, 2.28091430e-05,\n",
       "        1.42693107e-05, 2.66896791e-06, 1.51305285e-05, 4.21737286e-06,\n",
       "        8.43729958e-06, 8.00013440e-05, 1.82625052e-04, 5.56005834e-05,\n",
       "        5.07440127e-05, 2.41982707e-06, 2.09800291e-05, 2.76270330e-05,\n",
       "        9.29832750e-05, 3.28466122e-04, 8.75732439e-07, 3.33943972e-05,\n",
       "        1.25113665e-05, 1.93289088e-05, 2.61443165e-06, 5.15976571e-05,\n",
       "        5.22648770e-06, 3.28015478e-04, 8.24014387e-06, 1.91373438e-05,\n",
       "        3.87068394e-06, 3.84572806e-04, 8.11872360e-06, 1.75066998e-05,\n",
       "        1.51470304e-05, 9.83613336e-06, 1.72816071e-04, 8.49025037e-06,\n",
       "        2.89256974e-07, 1.03808327e-06, 8.80453808e-05, 7.57324102e-04,\n",
       "        4.96325083e-04, 8.79310483e-06, 4.99127746e-05, 2.72900297e-06,\n",
       "        7.74956425e-06, 9.64733954e-06, 2.32760121e-05, 9.40164227e-06,\n",
       "        3.72978480e-04, 1.10029971e-04, 4.90205537e-04, 5.06299839e-05,\n",
       "        5.63431007e-04, 2.92903715e-04, 2.33759401e-05, 1.02136873e-04,\n",
       "        1.91812046e-06, 1.63410948e-06, 3.22436936e-05, 1.41171849e-05,\n",
       "        2.87920935e-04, 9.60570742e-06, 4.70219791e-04, 8.24401195e-06,\n",
       "        1.21131752e-05, 3.17833001e-05, 5.43670376e-06, 4.28723230e-04,\n",
       "        7.59813411e-05, 1.42284771e-05, 9.87709882e-06, 1.23344880e-05,\n",
       "        8.92837579e-06, 1.49771938e-06, 5.29116551e-05, 2.60858383e-06,\n",
       "        4.49226536e-06, 5.75567867e-07, 9.75746752e-06, 2.48105964e-04,\n",
       "        5.47411364e-05, 5.08279209e-06, 1.10686433e-05, 1.57493523e-05,\n",
       "        4.88791033e-04, 1.43338286e-06, 2.06585664e-05, 1.01339374e-05,\n",
       "        7.31991531e-05, 1.82476538e-06, 5.33404818e-05, 6.40938524e-05,\n",
       "        2.79933483e-05, 1.01743048e-04, 2.13332114e-05, 1.06694858e-06,\n",
       "        2.00341441e-04, 1.59798656e-03, 1.75934310e-05, 1.50458402e-06,\n",
       "        1.10765213e-05, 5.54560174e-05, 9.87662770e-06, 1.26575958e-06,\n",
       "        1.17398258e-06, 1.22060846e-05, 6.16929773e-03, 3.08384129e-04,\n",
       "        4.79972106e-04, 1.61221988e-06, 5.63938215e-07, 2.55221297e-04,\n",
       "        1.09974462e-06, 4.15112572e-06, 1.03650336e-06, 1.27465910e-05,\n",
       "        1.34674929e-06, 4.58020950e-05, 1.35010896e-05, 4.77056565e-05,\n",
       "        2.98794521e-06, 9.09451774e-06, 2.42164606e-06, 2.42366309e-06,\n",
       "        2.78012476e-06, 1.69958166e-05, 9.31094200e-06, 4.60744189e-07,\n",
       "        1.18773736e-04, 4.51642990e-07, 2.91789604e-07, 7.03463820e-06,\n",
       "        3.53575288e-03, 8.91339278e-06, 1.70940893e-06, 2.13326421e-05,\n",
       "        1.58587363e-06, 2.78769653e-06, 1.61962089e-05, 1.21567653e-04,\n",
       "        7.53142695e-06, 8.80387051e-06, 3.75318450e-06, 3.58916077e-06,\n",
       "        3.42961562e-06, 1.53143219e-06, 1.59892932e-06, 7.48311413e-06,\n",
       "        1.62553351e-05, 2.73551850e-05, 3.21861944e-06, 7.39110419e-06,\n",
       "        6.86448242e-04, 8.14313025e-05, 2.34565232e-04, 2.31229533e-05,\n",
       "        4.18852142e-06, 9.72791167e-06, 6.46542685e-06, 1.27299472e-05,\n",
       "        3.55635094e-03, 2.47333919e-05, 9.25518107e-05, 1.51269924e-05,\n",
       "        6.75246019e-06, 2.82125802e-05, 3.65687883e-05, 3.77997465e-04,\n",
       "        2.97472016e-05, 2.30664609e-05, 6.69558347e-07, 2.13866628e-07,\n",
       "        5.04809068e-05, 3.04484411e-06, 4.16801049e-05, 3.36535027e-06,\n",
       "        2.44734747e-06, 7.84157528e-05, 1.83200871e-06, 8.94675122e-06,\n",
       "        9.76643605e-07, 3.32191712e-06, 1.00598936e-06, 1.62051924e-06,\n",
       "        4.56999624e-05, 1.06264393e-04, 1.18712001e-06, 1.15190051e-06,\n",
       "        3.24468942e-06, 1.64997019e-07, 2.82865744e-06, 4.27629459e-07,\n",
       "        1.39743474e-06, 7.52593915e-05, 1.02036961e-06, 6.20573792e-06,\n",
       "        7.50621837e-07, 2.31395404e-07, 5.03064257e-06, 1.04854439e-06,\n",
       "        7.99428392e-07, 9.80698019e-07, 1.90712257e-07, 2.09009772e-06,\n",
       "        1.03705270e-05, 1.15656621e-05, 2.33986448e-05, 2.23270445e-06,\n",
       "        2.00191316e-05, 7.80433584e-06, 2.45298233e-05, 2.32999196e-06,\n",
       "        6.73305635e-07, 3.50087430e-05, 4.14610695e-05, 1.26038731e-06,\n",
       "        1.39403319e-05, 1.70573912e-05, 1.42799010e-07, 5.29620877e-07,\n",
       "        3.28543450e-07, 3.07756954e-06, 1.06962025e-05, 2.21843311e-06,\n",
       "        1.93701315e-04, 3.16377452e-07, 7.45751004e-06, 1.69223116e-04,\n",
       "        4.13496900e-06, 3.34273568e-06, 3.82197795e-05, 1.40479574e-06,\n",
       "        5.31526211e-06, 1.17018717e-05, 1.47666597e-05, 6.37334949e-07,\n",
       "        1.87723401e-06, 4.31460239e-06, 2.82242468e-06, 4.44168654e-06,\n",
       "        6.13318070e-06, 1.97255827e-06, 5.34779531e-07, 2.40886839e-05,\n",
       "        2.33835744e-04, 2.79873693e-05, 3.83128872e-06, 5.63573678e-07,\n",
       "        5.16927834e-08, 3.75465731e-07, 9.68505674e-07, 9.82775600e-05,\n",
       "        2.15798835e-07, 2.47982484e-07, 1.06634079e-05, 1.30275977e-04]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# получили список из 1000 \"вероятностей\"\n",
    "# сейчас не очень понятно, какая цифра за что отвечает\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('n01622779', 'great_grey_owl', 0.2900877),\n",
       "  ('n02123045', 'tabby', 0.22539608),\n",
       "  ('n02108915', 'French_bulldog', 0.120665394),\n",
       "  ('n02124075', 'Egyptian_cat', 0.09189093),\n",
       "  ('n02123159', 'tiger_cat', 0.032158583)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# чтобы расшифровать это дело, тоже есть специальная функция\n",
    "\n",
    "tf.keras.applications.resnet.decode_predictions(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И как мы видим, самым вероятным классом внезапно оказалась \"сова\". Более того, в топе присутствует несколько отдельных пород кошек (и одна собака), но даже не все они содержат слово cat в названии. В то время как нам нужно просто отвечать на вопрос \"кот или не кот\". \n",
    "\n",
    "Вместо того, чтобы пытаться интерпретировать полученные результаты, которые ещё и не всегда точные, есть смысл попробовать дообучить последние слои на именно наших данных, а вдобавок навесить на него ещё и дополнительный слой из 1 нейрона для бинарной классификации.\n",
    "\n",
    "Чтобы это сделать, сперва стоит заморозить все слои, кроме последних. Затем у последнего заменим функцию активации на ReLU (сейчас там стоит Softmax, т.к. это был последний слой в мультиклассовой классификации). Ну и наконец навесим на всё это дело слой из 1 нейрона для бинарной классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заморозим все слои\n",
    "\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# но несколько последних разморозим обратно\n",
    "# число 20 здесь взято для примера, а вообще, конечно,\n",
    "# это тоже гиперпараметр, который нужно подбирать\n",
    "# оптимальным может оказаться любое значение от 1 слоя до переобучения половины сети\n",
    "\n",
    "for layer in resnet.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# заменим активацию на последнем слое\n",
    "\n",
    "resnet.layers[-1].activation = tf.keras.activations.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 1000)         2049000     avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 25,636,712\n",
      "Trainable params: 9,930,216\n",
      "Non-trainable params: 15,706,496\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# посмотрим, что получилось\n",
    "\n",
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сделаем новую модель на базе старой. Просто включим весь наш resnet в качестве слоя и добавим к нему всё необходимое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cats = tf.keras.models.Sequential([\n",
    "    resnet,  # вся модель выступает в качестве слоя\n",
    "    tf.keras.layers.BatchNormalization(),  # почему бы и нет?\n",
    "    tf.keras.layers.Dropout(0.5),  # по-хорошему, дропаут тоже нужно подбирать\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # слой для бинарной классификации\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Functional)        (None, 1000)              25636712  \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 25,641,713\n",
      "Trainable params: 9,933,217\n",
      "Non-trainable params: 15,708,496\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# что в итоге?\n",
    "\n",
    "model_cats.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# скомпилируем получившуюся модель, добавив необходимые метрики\n",
    "\n",
    "accuracy = tf.keras.metrics.binary_accuracy\n",
    "precision = tf.keras.metrics.Precision()\n",
    "recall = tf.keras.metrics.Recall()\n",
    "\n",
    "# как и в прошлый раз, F1 напишем сами\n",
    "def f1_metrics(y_true, y_pred):\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    return 2 * ((prec * rec) / (prec + rec + 1e-7))\n",
    "\n",
    "\n",
    "model_cats.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "                   loss=tf.keras.losses.binary_crossentropy,\n",
    "                   metrics=[accuracy, precision, recall, f1_metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно сформировать датасет для тренировки модели. Наши картинки располагаются в папке `pics`. Как их подготавливать для нейросети, мы уже знаем. Поэтому давайте перегоним их все в массивы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция предподготовки картинки для модели\n",
    "\n",
    "def preprocess_image(file):\n",
    "    img = tf.keras.preprocessing.image.load_img(file, target_size=(224, 224))  # загружаем в нужном разрешении\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)  # конвертируем в массив\n",
    "    img = tf.keras.applications.resnet.preprocess_input(img)  # препроцессинг для resnet\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробежимся по всем файлам в наших папках и добавим их в соответствующие списки. К каждой картинке добавим лейбл: если кот - 1, в иных случаях - 0. Это поможет не запутсаться в данных при перемешивании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавляем пары (картинка, 1) для картинок с котами\n",
    "cats = [(preprocess_image('pics/cats/'+file), 1) for file in os.listdir('pics/cats')]\n",
    "\n",
    "# и пары (картинка, 0) для картинок без котов\n",
    "nocats = [(preprocess_image('pics/nocats/'+file), 0) for file in os.listdir('pics/nocats')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сливаем оба списка вместе\n",
    "\n",
    "all_pics = cats + nocats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# и перемешиваем данные\n",
    "\n",
    "random.shuffle(all_pics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в x отправляем картинки, а в y - прикреплённые к ним лейблы\n",
    "\n",
    "x = np.array([a[0] for a in all_pics])\n",
    "y = np.array([a[1] for a in all_pics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# делим данные на трейн, валидацию и тест традиционным образом\n",
    "\n",
    "def train_val_test_split(x, val_frac=0.15, test_frac=0.15):\n",
    "    x_train = x[:round((1 - val_frac - test_frac) * len(x))]\n",
    "    x_val = x[round((1 - val_frac - test_frac) * len(x)):round((1 - test_frac) * len(x))]\n",
    "    x_test = x[round((1 - test_frac) * len(x)):]\n",
    "    return x_train, x_val, x_test\n",
    "\n",
    "\n",
    "x_train, x_val, x_test = train_val_test_split(x)\n",
    "y_train, y_val, y_test = train_val_test_split(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Keras уже есть инструмент для выполнения простеньких онлайн-аугментаций: `ImageDataGenerator`. Воспользуемся им, чтобы применять к картинкам случайные трансформации в процессе обучения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# настроек у этого класса куда больше, но для примера возьмём только самые основные \n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=45,  # случайный поворот в пределах 45 градусов\n",
    "                                                          width_shift_range=0.2,  # случайный сдвиг по горизонтали\n",
    "                                                          height_shift_range=0.2,  # и вертикали\n",
    "                                                          horizontal_flip=True,  # случайное отражение по горизонтали\n",
    "                                                          vertical_flip=True)  # и вертикали\n",
    "\n",
    "# также у ImageDataGenerator есть полезный аргумент preprocessing_function,\n",
    "# в котором можно указать любую свою функцию для обработки изображения\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# будем отслеживать обучение в Tensorboard\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir='logs/tl_resnet_cats', histogram_freq=1)\n",
    "\n",
    "# и уменьшать lr на плато\n",
    "\n",
    "annealing = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 1/10 [=>............................] - ETA: 0s - loss: 1.3025 - binary_accuracy: 0.2500 - precision: 0.1765 - recall: 0.2308 - f1_metrics: 0.2000WARNING:tensorflow:From c:\\users\\g0nzalez\\venv38\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "11/10 [================================] - 10s 892ms/step - loss: 1.1410 - binary_accuracy: 0.4455 - precision: 0.3165 - recall: 0.4167 - f1_metrics: 0.3151 - val_loss: 1.2474 - val_binary_accuracy: 0.4706 - val_precision: 0.3667 - val_recall: 0.3929 - val_f1_metrics: 0.4014\n",
      "Epoch 2/50\n",
      "11/10 [================================] - 6s 573ms/step - loss: 0.9592 - binary_accuracy: 0.5327 - precision: 0.4051 - recall: 0.5333 - f1_metrics: 0.4162 - val_loss: 1.0484 - val_binary_accuracy: 0.5000 - val_precision: 0.4118 - val_recall: 0.5000 - val_f1_metrics: 0.4449\n",
      "Epoch 3/50\n",
      "11/10 [================================] - 7s 597ms/step - loss: 0.8865 - binary_accuracy: 0.5358 - precision: 0.4162 - recall: 0.6000 - f1_metrics: 0.4864 - val_loss: 0.9170 - val_binary_accuracy: 0.5735 - val_precision: 0.4878 - val_recall: 0.7143 - val_f1_metrics: 0.5676\n",
      "Epoch 4/50\n",
      "11/10 [================================] - 7s 595ms/step - loss: 0.8334 - binary_accuracy: 0.5826 - precision: 0.4545 - recall: 0.5833 - f1_metrics: 0.4824 - val_loss: 0.8148 - val_binary_accuracy: 0.6029 - val_precision: 0.5116 - val_recall: 0.7857 - val_f1_metrics: 0.5950\n",
      "Epoch 5/50\n",
      "11/10 [================================] - 7s 649ms/step - loss: 0.7962 - binary_accuracy: 0.6168 - precision: 0.4909 - recall: 0.6750 - f1_metrics: 0.5353 - val_loss: 0.7392 - val_binary_accuracy: 0.6029 - val_precision: 0.5111 - val_recall: 0.8214 - val_f1_metrics: 0.6131\n",
      "Epoch 6/50\n",
      "11/10 [================================] - 7s 635ms/step - loss: 0.7041 - binary_accuracy: 0.6355 - precision: 0.5096 - recall: 0.6667 - f1_metrics: 0.5588 - val_loss: 0.6952 - val_binary_accuracy: 0.6618 - val_precision: 0.5532 - val_recall: 0.9286 - val_f1_metrics: 0.6563\n",
      "Epoch 7/50\n",
      "11/10 [================================] - 7s 611ms/step - loss: 0.6282 - binary_accuracy: 0.6729 - precision: 0.5478 - recall: 0.7167 - f1_metrics: 0.5765 - val_loss: 0.6523 - val_binary_accuracy: 0.6618 - val_precision: 0.5532 - val_recall: 0.9286 - val_f1_metrics: 0.6563\n",
      "Epoch 8/50\n",
      "11/10 [================================] - 7s 602ms/step - loss: 0.6691 - binary_accuracy: 0.6698 - precision: 0.5455 - recall: 0.7000 - f1_metrics: 0.6147 - val_loss: 0.6258 - val_binary_accuracy: 0.6618 - val_precision: 0.5532 - val_recall: 0.9286 - val_f1_metrics: 0.6563\n",
      "Epoch 9/50\n",
      "11/10 [================================] - 8s 751ms/step - loss: 0.5844 - binary_accuracy: 0.7165 - precision: 0.5924 - recall: 0.7750 - f1_metrics: 0.6555 - val_loss: 0.5864 - val_binary_accuracy: 0.6765 - val_precision: 0.5652 - val_recall: 0.9286 - val_f1_metrics: 0.6699\n",
      "Epoch 10/50\n",
      "11/10 [================================] - 7s 611ms/step - loss: 0.4900 - binary_accuracy: 0.7445 - precision: 0.6203 - recall: 0.8167 - f1_metrics: 0.7151 - val_loss: 0.5655 - val_binary_accuracy: 0.7059 - val_precision: 0.5870 - val_recall: 0.9643 - val_f1_metrics: 0.7131\n",
      "Epoch 11/50\n",
      "11/10 [================================] - 7s 597ms/step - loss: 0.4470 - binary_accuracy: 0.8006 - precision: 0.7059 - recall: 0.8000 - f1_metrics: 0.8178 - val_loss: 0.5515 - val_binary_accuracy: 0.7059 - val_precision: 0.5870 - val_recall: 0.9643 - val_f1_metrics: 0.7131\n",
      "Epoch 12/50\n",
      "11/10 [================================] - 7s 630ms/step - loss: 0.4449 - binary_accuracy: 0.7975 - precision: 0.6871 - recall: 0.8417 - f1_metrics: 0.7046 - val_loss: 0.5345 - val_binary_accuracy: 0.7059 - val_precision: 0.5870 - val_recall: 0.9643 - val_f1_metrics: 0.7131\n",
      "Epoch 13/50\n",
      "11/10 [================================] - 7s 613ms/step - loss: 0.5118 - binary_accuracy: 0.7819 - precision: 0.6645 - recall: 0.8417 - f1_metrics: 0.7137 - val_loss: 0.5105 - val_binary_accuracy: 0.7059 - val_precision: 0.5870 - val_recall: 0.9643 - val_f1_metrics: 0.7131\n",
      "Epoch 14/50\n",
      "11/10 [================================] - 7s 672ms/step - loss: 0.3708 - binary_accuracy: 0.8162 - precision: 0.7020 - recall: 0.8833 - f1_metrics: 0.8176 - val_loss: 0.4879 - val_binary_accuracy: 0.7059 - val_precision: 0.5870 - val_recall: 0.9643 - val_f1_metrics: 0.7131\n",
      "Epoch 15/50\n",
      "11/10 [================================] - 8s 703ms/step - loss: 0.3697 - binary_accuracy: 0.8442 - precision: 0.7574 - recall: 0.8583 - f1_metrics: 0.7814 - val_loss: 0.4516 - val_binary_accuracy: 0.7206 - val_precision: 0.6000 - val_recall: 0.9643 - val_f1_metrics: 0.7286\n",
      "Epoch 16/50\n",
      "11/10 [================================] - 7s 611ms/step - loss: 0.3397 - binary_accuracy: 0.8598 - precision: 0.7622 - recall: 0.9083 - f1_metrics: 0.8187 - val_loss: 0.4263 - val_binary_accuracy: 0.7500 - val_precision: 0.6279 - val_recall: 0.9643 - val_f1_metrics: 0.7430\n",
      "Epoch 17/50\n",
      "11/10 [================================] - 7s 643ms/step - loss: 0.3420 - binary_accuracy: 0.8380 - precision: 0.7429 - recall: 0.8667 - f1_metrics: 0.7140 - val_loss: 0.4083 - val_binary_accuracy: 0.7647 - val_precision: 0.6429 - val_recall: 0.9643 - val_f1_metrics: 0.7505\n",
      "Epoch 18/50\n",
      "11/10 [================================] - 7s 662ms/step - loss: 0.3627 - binary_accuracy: 0.8567 - precision: 0.7569 - recall: 0.9083 - f1_metrics: 0.8352 - val_loss: 0.3905 - val_binary_accuracy: 0.8088 - val_precision: 0.6923 - val_recall: 0.9643 - val_f1_metrics: 0.7838\n",
      "Epoch 19/50\n",
      "11/10 [================================] - 7s 626ms/step - loss: 0.3078 - binary_accuracy: 0.8816 - precision: 0.8203 - recall: 0.8750 - f1_metrics: 0.8507 - val_loss: 0.3713 - val_binary_accuracy: 0.8235 - val_precision: 0.7105 - val_recall: 0.9643 - val_f1_metrics: 0.8023\n",
      "Epoch 20/50\n",
      "11/10 [================================] - 7s 597ms/step - loss: 0.2920 - binary_accuracy: 0.8660 - precision: 0.8130 - recall: 0.8333 - f1_metrics: 0.8609 - val_loss: 0.3580 - val_binary_accuracy: 0.8235 - val_precision: 0.7105 - val_recall: 0.9643 - val_f1_metrics: 0.8023\n",
      "Epoch 21/50\n",
      "11/10 [================================] - 7s 609ms/step - loss: 0.2977 - binary_accuracy: 0.8692 - precision: 0.8047 - recall: 0.8583 - f1_metrics: 0.7887 - val_loss: 0.3333 - val_binary_accuracy: 0.8382 - val_precision: 0.7297 - val_recall: 0.9643 - val_f1_metrics: 0.8219\n",
      "Epoch 22/50\n",
      "11/10 [================================] - 7s 656ms/step - loss: 0.2715 - binary_accuracy: 0.8847 - precision: 0.8217 - recall: 0.8833 - f1_metrics: 0.8547 - val_loss: 0.3186 - val_binary_accuracy: 0.8529 - val_precision: 0.7500 - val_recall: 0.9643 - val_f1_metrics: 0.8429\n",
      "Epoch 23/50\n",
      "11/10 [================================] - 7s 631ms/step - loss: 0.3116 - binary_accuracy: 0.8785 - precision: 0.8092 - recall: 0.8833 - f1_metrics: 0.7970 - val_loss: 0.2994 - val_binary_accuracy: 0.8529 - val_precision: 0.7647 - val_recall: 0.9286 - val_f1_metrics: 0.8526\n",
      "Epoch 24/50\n",
      "11/10 [================================] - 7s 660ms/step - loss: 0.2614 - binary_accuracy: 0.8879 - precision: 0.8333 - recall: 0.8750 - f1_metrics: 0.8809 - val_loss: 0.2834 - val_binary_accuracy: 0.9118 - val_precision: 0.8667 - val_recall: 0.9286 - val_f1_metrics: 0.8965\n",
      "Epoch 25/50\n",
      "11/10 [================================] - 7s 600ms/step - loss: 0.2989 - binary_accuracy: 0.8474 - precision: 0.7795 - recall: 0.8250 - f1_metrics: 0.7948 - val_loss: 0.2687 - val_binary_accuracy: 0.9118 - val_precision: 0.8667 - val_recall: 0.9286 - val_f1_metrics: 0.8965\n",
      "Epoch 26/50\n",
      "11/10 [================================] - 7s 598ms/step - loss: 0.2221 - binary_accuracy: 0.9065 - precision: 0.8571 - recall: 0.9000 - f1_metrics: 0.8805 - val_loss: 0.2630 - val_binary_accuracy: 0.9118 - val_precision: 0.8667 - val_recall: 0.9286 - val_f1_metrics: 0.8965\n",
      "Epoch 27/50\n",
      "11/10 [================================] - 7s 612ms/step - loss: 0.2217 - binary_accuracy: 0.9159 - precision: 0.8605 - recall: 0.9250 - f1_metrics: 0.8865 - val_loss: 0.2595 - val_binary_accuracy: 0.9118 - val_precision: 0.8667 - val_recall: 0.9286 - val_f1_metrics: 0.8965\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10 [================================] - 7s 620ms/step - loss: 0.2451 - binary_accuracy: 0.9065 - precision: 0.8750 - recall: 0.8750 - f1_metrics: 0.8617 - val_loss: 0.2447 - val_binary_accuracy: 0.9265 - val_precision: 0.8966 - val_recall: 0.9286 - val_f1_metrics: 0.9071\n",
      "Epoch 29/50\n",
      "11/10 [================================] - 7s 608ms/step - loss: 0.2284 - binary_accuracy: 0.9003 - precision: 0.8492 - recall: 0.8917 - f1_metrics: 0.8402 - val_loss: 0.2335 - val_binary_accuracy: 0.9412 - val_precision: 0.9286 - val_recall: 0.9286 - val_f1_metrics: 0.9340\n",
      "Epoch 30/50\n",
      "11/10 [================================] - 7s 596ms/step - loss: 0.2039 - binary_accuracy: 0.8972 - precision: 0.8425 - recall: 0.8917 - f1_metrics: 0.8617 - val_loss: 0.2190 - val_binary_accuracy: 0.9412 - val_precision: 0.9286 - val_recall: 0.9286 - val_f1_metrics: 0.9340\n",
      "Epoch 31/50\n",
      "11/10 [================================] - 7s 598ms/step - loss: 0.1867 - binary_accuracy: 0.9377 - precision: 0.9237 - recall: 0.9083 - f1_metrics: 0.9317 - val_loss: 0.2085 - val_binary_accuracy: 0.9412 - val_precision: 0.9286 - val_recall: 0.9286 - val_f1_metrics: 0.9340\n",
      "Epoch 32/50\n",
      "11/10 [================================] - 6s 591ms/step - loss: 0.2084 - binary_accuracy: 0.9065 - precision: 0.8629 - recall: 0.8917 - f1_metrics: 0.8951 - val_loss: 0.1978 - val_binary_accuracy: 0.9412 - val_precision: 0.9286 - val_recall: 0.9286 - val_f1_metrics: 0.9340\n",
      "Epoch 33/50\n",
      "11/10 [================================] - 7s 595ms/step - loss: 0.1836 - binary_accuracy: 0.9190 - precision: 0.8730 - recall: 0.9167 - f1_metrics: 0.8955 - val_loss: 0.1864 - val_binary_accuracy: 0.9412 - val_precision: 0.9286 - val_recall: 0.9286 - val_f1_metrics: 0.9340\n",
      "Epoch 34/50\n",
      "11/10 [================================] - 6s 578ms/step - loss: 0.1720 - binary_accuracy: 0.9377 - precision: 0.9167 - recall: 0.9167 - f1_metrics: 0.9252 - val_loss: 0.1798 - val_binary_accuracy: 0.9412 - val_precision: 0.9615 - val_recall: 0.8929 - val_f1_metrics: 0.9126\n",
      "Epoch 35/50\n",
      "11/10 [================================] - 7s 595ms/step - loss: 0.2166 - binary_accuracy: 0.9128 - precision: 0.8651 - recall: 0.9083 - f1_metrics: 0.8829 - val_loss: 0.1753 - val_binary_accuracy: 0.9412 - val_precision: 0.9615 - val_recall: 0.8929 - val_f1_metrics: 0.9126\n",
      "Epoch 36/50\n",
      "11/10 [================================] - 6s 572ms/step - loss: 0.1518 - binary_accuracy: 0.9470 - precision: 0.9256 - recall: 0.9333 - f1_metrics: 0.9333 - val_loss: 0.1749 - val_binary_accuracy: 0.9412 - val_precision: 0.9615 - val_recall: 0.8929 - val_f1_metrics: 0.9126\n",
      "Epoch 37/50\n",
      "11/10 [================================] - 6s 584ms/step - loss: 0.1824 - binary_accuracy: 0.9377 - precision: 0.8968 - recall: 0.9417 - f1_metrics: 0.9181 - val_loss: 0.1723 - val_binary_accuracy: 0.9412 - val_precision: 0.9615 - val_recall: 0.8929 - val_f1_metrics: 0.9126\n",
      "Epoch 38/50\n",
      "11/10 [================================] - 7s 606ms/step - loss: 0.1420 - binary_accuracy: 0.9564 - precision: 0.9274 - recall: 0.9583 - f1_metrics: 0.9415 - val_loss: 0.1658 - val_binary_accuracy: 0.9412 - val_precision: 0.9615 - val_recall: 0.8929 - val_f1_metrics: 0.9126\n",
      "Epoch 39/50\n",
      "11/10 [================================] - 7s 601ms/step - loss: 0.1559 - binary_accuracy: 0.9315 - precision: 0.8889 - recall: 0.9333 - f1_metrics: 0.9138 - val_loss: 0.1637 - val_binary_accuracy: 0.9412 - val_precision: 0.9615 - val_recall: 0.8929 - val_f1_metrics: 0.9126\n",
      "Epoch 40/50\n",
      "11/10 [================================] - 7s 600ms/step - loss: 0.1540 - binary_accuracy: 0.9283 - precision: 0.9217 - recall: 0.8833 - f1_metrics: 0.8949 - val_loss: 0.1601 - val_binary_accuracy: 0.9412 - val_precision: 0.9615 - val_recall: 0.8929 - val_f1_metrics: 0.9126\n",
      "Epoch 41/50\n",
      "11/10 [================================] - 7s 598ms/step - loss: 0.1346 - binary_accuracy: 0.9439 - precision: 0.9180 - recall: 0.9333 - f1_metrics: 0.9430 - val_loss: 0.1566 - val_binary_accuracy: 0.9412 - val_precision: 0.9615 - val_recall: 0.8929 - val_f1_metrics: 0.9126\n",
      "Epoch 42/50\n",
      "11/10 [================================] - 7s 620ms/step - loss: 0.1811 - binary_accuracy: 0.9346 - precision: 0.9091 - recall: 0.9167 - f1_metrics: 0.9376 - val_loss: 0.1556 - val_binary_accuracy: 0.9412 - val_precision: 0.9615 - val_recall: 0.8929 - val_f1_metrics: 0.9126\n",
      "Epoch 43/50\n",
      "11/10 [================================] - 7s 600ms/step - loss: 0.1255 - binary_accuracy: 0.9564 - precision: 0.9344 - recall: 0.9500 - f1_metrics: 0.9376 - val_loss: 0.1529 - val_binary_accuracy: 0.9412 - val_precision: 0.9615 - val_recall: 0.8929 - val_f1_metrics: 0.9126\n",
      "Epoch 44/50\n",
      "11/10 [================================] - 7s 615ms/step - loss: 0.1302 - binary_accuracy: 0.9502 - precision: 0.9194 - recall: 0.9500 - f1_metrics: 0.9309 - val_loss: 0.1533 - val_binary_accuracy: 0.9412 - val_precision: 0.9615 - val_recall: 0.8929 - val_f1_metrics: 0.9126\n",
      "Epoch 45/50\n",
      "11/10 [================================] - 7s 614ms/step - loss: 0.1574 - binary_accuracy: 0.9346 - precision: 0.9160 - recall: 0.9083 - f1_metrics: 0.9332 - val_loss: 0.1516 - val_binary_accuracy: 0.9412 - val_precision: 0.9615 - val_recall: 0.8929 - val_f1_metrics: 0.9126\n",
      "Epoch 46/50\n",
      "11/10 [================================] - 7s 610ms/step - loss: 0.1331 - binary_accuracy: 0.9595 - precision: 0.9496 - recall: 0.9417 - f1_metrics: 0.9312 - val_loss: 0.1505 - val_binary_accuracy: 0.9412 - val_precision: 0.9615 - val_recall: 0.8929 - val_f1_metrics: 0.9126\n",
      "Epoch 47/50\n",
      "11/10 [================================] - 7s 616ms/step - loss: 0.0990 - binary_accuracy: 0.9751 - precision: 0.9746 - recall: 0.9583 - f1_metrics: 0.9628 - val_loss: 0.1513 - val_binary_accuracy: 0.9412 - val_precision: 0.9615 - val_recall: 0.8929 - val_f1_metrics: 0.9126\n",
      "Epoch 48/50\n",
      "11/10 [================================] - 6s 584ms/step - loss: 0.1048 - binary_accuracy: 0.9657 - precision: 0.9504 - recall: 0.9583 - f1_metrics: 0.9577 - val_loss: 0.1516 - val_binary_accuracy: 0.9412 - val_precision: 0.9615 - val_recall: 0.8929 - val_f1_metrics: 0.9126\n",
      "Epoch 49/50\n",
      "11/10 [================================] - 6s 581ms/step - loss: 0.1000 - binary_accuracy: 0.9657 - precision: 0.9504 - recall: 0.9583 - f1_metrics: 0.9581 - val_loss: 0.1509 - val_binary_accuracy: 0.9412 - val_precision: 0.9615 - val_recall: 0.8929 - val_f1_metrics: 0.9126\n",
      "Epoch 50/50\n",
      "11/10 [================================] - 7s 612ms/step - loss: 0.1098 - binary_accuracy: 0.9626 - precision: 0.9500 - recall: 0.9500 - f1_metrics: 0.9433 - val_loss: 0.1505 - val_binary_accuracy: 0.9412 - val_precision: 0.9615 - val_recall: 0.8929 - val_f1_metrics: 0.9126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24aa97148b0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 32  # размер батча\n",
    "\n",
    "# вместо самих данных подаём в цикл обучения картинки из нашего генератора\n",
    "model_cats.fit(datagen.flow(x_train, y_train, batch_size=bs),  # обратите внимание, что размер батча указывается тут\n",
    "               validation_data=(x_val, y_val),\n",
    "               steps_per_epoch=len(x_train)/bs,  # чтобы генератор не уходил в бесконечный цикл, указываем количество шагов\n",
    "               epochs=50,\n",
    "               callbacks=[tb_callback, annealing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 348ms/step - loss: 0.1351 - binary_accuracy: 0.9420 - precision: 0.8889 - recall: 0.9600 - f1_metrics: 0.9236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13511879742145538,\n",
       " 0.9420289993286133,\n",
       " 0.8888888955116272,\n",
       " 0.9599999785423279,\n",
       " 0.9236477017402649]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверим модель на тестовой выборке\n",
    "\n",
    "model_cats.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты вполне себе неплохие, хоть и чуть отличаются от таковых на валидации. Но это можно объяснить небольшим размером датасета и тестовой+валидационной выборки в частности. Когда данных мало, отличия \"в среднем\" могут быть более заметны.\n",
    "\n",
    "Давайте теперь посмотрим, определит ли наша модель, что на изначальной картинке всё-таки кошка, а не сова..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98047835]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cats.predict(cat_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь модель говорит, что на нашей картинке с 98%-ной вероятностью кошка. \n",
    "\n",
    "Таким образом, при помощи transfer learning предобученной свёрточной нейросети нам удалось дообучить нейросеть для решения более узкой задачи. Конечно, результат ещё можно продолжать улучшать, подбирая гиперпараметры и улучшая аугментации, но с этим вы уже можете справиться и самостоятельно =)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
