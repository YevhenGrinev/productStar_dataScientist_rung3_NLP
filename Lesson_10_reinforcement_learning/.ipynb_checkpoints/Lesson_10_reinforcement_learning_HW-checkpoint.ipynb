{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# фиксируем сид\n",
    "np.random.seed(17)\n",
    "tf.random.set_seed(17)\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# GPUs = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(GPUs[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_inputs = 4  \n",
    "num_actions = 2 \n",
    "\n",
    "num_hidden = 64\n",
    "\n",
    "inputs = layers.Input(shape=(num_inputs,))\n",
    "common1 = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "common2 = layers.Dense(num_hidden, activation=\"relu\")(common1)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common2)\n",
    "critic = layers.Dense(1)(common2)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.legacy.Adam(learning_rate=0.01)\n",
    "loss = keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           320         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           4160        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 2)            130         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            65          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,675\n",
      "Trainable params: 4,675\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, running reward: 8.75\n",
      "Episode: 20, running reward: 12.45\n",
      "Episode: 30, running reward: 15.14\n",
      "Episode: 40, running reward: 13.79\n",
      "Episode: 50, running reward: 13.58\n",
      "Episode: 60, running reward: 14.32\n",
      "Episode: 70, running reward: 15.23\n",
      "Episode: 80, running reward: 14.28\n",
      "Episode: 90, running reward: 15.10\n",
      "Episode: 100, running reward: 16.55\n",
      "Episode: 110, running reward: 16.36\n",
      "Episode: 120, running reward: 16.07\n",
      "Episode: 130, running reward: 18.55\n",
      "Episode: 140, running reward: 30.46\n",
      "Episode: 150, running reward: 32.53\n",
      "Episode: 160, running reward: 34.77\n",
      "Episode: 170, running reward: 39.02\n",
      "Episode: 180, running reward: 38.96\n",
      "Episode: 190, running reward: 29.16\n",
      "Episode: 200, running reward: 23.60\n",
      "Episode: 210, running reward: 19.51\n",
      "Episode: 220, running reward: 17.07\n",
      "Episode: 230, running reward: 15.36\n",
      "Episode: 240, running reward: 19.01\n",
      "Episode: 250, running reward: 23.60\n",
      "Episode: 260, running reward: 28.36\n",
      "Episode: 270, running reward: 34.11\n",
      "Episode: 280, running reward: 34.38\n",
      "Episode: 290, running reward: 34.98\n",
      "Episode: 300, running reward: 36.81\n",
      "Episode: 310, running reward: 38.90\n",
      "Episode: 320, running reward: 40.94\n",
      "Episode: 330, running reward: 47.96\n",
      "Episode: 340, running reward: 58.12\n",
      "Episode: 350, running reward: 57.55\n",
      "Episode: 360, running reward: 47.18\n",
      "Episode: 370, running reward: 45.03\n",
      "Episode: 380, running reward: 60.33\n",
      "Episode: 390, running reward: 58.90\n",
      "Episode: 400, running reward: 56.89\n",
      "Episode: 410, running reward: 66.53\n",
      "Episode: 420, running reward: 89.21\n",
      "Episode: 430, running reward: 102.02\n",
      "Episode: 440, running reward: 90.96\n",
      "Episode: 450, running reward: 84.86\n",
      "Episode: 460, running reward: 104.24\n",
      "Episode: 470, running reward: 103.28\n",
      "Episode: 480, running reward: 102.74\n",
      "Episode: 490, running reward: 101.91\n",
      "Episode: 500, running reward: 90.48\n",
      "Episode: 510, running reward: 75.19\n",
      "Episode: 520, running reward: 68.19\n",
      "Episode: 530, running reward: 73.99\n",
      "Episode: 540, running reward: 81.84\n",
      "Episode: 550, running reward: 77.66\n",
      "Episode: 560, running reward: 78.30\n",
      "Episode: 570, running reward: 71.83\n",
      "Episode: 580, running reward: 68.81\n",
      "Episode: 590, running reward: 94.82\n",
      "Episode: 600, running reward: 127.63\n",
      "Episode: 610, running reward: 161.00\n",
      "Episode: 620, running reward: 162.78\n",
      "Episode: 630, running reward: 129.89\n",
      "Episode: 640, running reward: 102.37\n",
      "Episode: 650, running reward: 85.23\n",
      "Episode: 660, running reward: 82.06\n",
      "Episode: 670, running reward: 97.27\n",
      "Episode: 680, running reward: 107.69\n",
      "Episode: 690, running reward: 118.12\n",
      "Episode: 700, running reward: 121.20\n",
      "Episode: 710, running reward: 115.70\n",
      "Episode: 720, running reward: 114.60\n",
      "Episode: 730, running reward: 114.33\n",
      "Episode: 740, running reward: 121.59\n",
      "Episode: 750, running reward: 146.94\n",
      "Solved at episode 753!\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.995\n",
    "\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "while True:\n",
    "    state = env.reset(seed=17)\n",
    "    state = state[0]\n",
    "    episode_reward = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            env.render()\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "            \n",
    "            action_probs, critic_value = model(state)\n",
    "            \n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            \n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "\n",
    "            state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            rewards_history.append(reward)\n",
    "            \n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            \n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "            critic_losses.append(\n",
    "                loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "            )\n",
    "\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "    episode_count += 1\n",
    "    \n",
    "    if episode_count % 10 == 0:\n",
    "        print(f\"Episode: {episode_count}, running reward: {running_reward:.2f}\")\n",
    "\n",
    "    if running_reward > 195:\n",
    "        print(f\"Solved at episode {episode_count}!\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим теперь, как натренированная нейросеть играет с нашей платформой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_test_timestaps = 100000\n",
    "test_episodes = 5\n",
    "\n",
    "for ep in range(test_episodes):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    for timestap in range(max_test_timestaps):\n",
    "      \n",
    "        env.render()\n",
    "        \n",
    "        state = tf.convert_to_tensor(state)\n",
    "        state = tf.expand_dims(state, 0)\n",
    "\n",
    "        action_probs, _ = model(state)\n",
    "\n",
    "        # здесь уже можно выбирать конкретное действие\n",
    "        if action_probs[0][0].numpy() >= 0.5:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 1\n",
    "\n",
    "        state, _, done, _, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
